---
layout: post
title: Spectral Clustering From Scratch
---

In this blog post, we will be creating a simple version of the Spectral Clustering algorithm using Python.   

## What is Clustering?

*Clustering* refers to the task of separating a data set into a certain number of groups based on similarity between data points. *K-means* is perhaps the most common way to achieve this task which uses a distance metric relative to a centroid to cluster data. 

We will be using *Spectral Clustering* which looks at data points in the data as a connected graph and then partitions the graph into subgraphs i.e the clusters. This method is more suited for data which is shaped 'weird' as shown below:

```python
# import these packages
import numpy as np
from sklearn import datasets
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
```

- ### A simple clustering problem
 
```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)

km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
{::options parse_block_html="true" /}
<div class="red-box">

- <b> `datasets.make_blogs()` </b>: This function creates a dataset of clusters based several parameters. 
It returns:
1. <b> X </b>: a $$ \mathbf{R}^{n \hspace{0.08cm} X \hspace{0.08cm} 2} $$ array representing points in the `2D` plane
2. <b> y </b>: the true cluster labels for each point in X 
  
- `KMeans()` : creates a k-means clustering model 
</div>
{::options parse_block_html="false" /}

![title](/images/output_4_1.png)


- ### A complex clustering problem

```python
np.random.seed(1234)
n = 200 

X,y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None) 

km = KMeans(n_clusters = 2) 
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![title](/images/output_8_1.png)

We can see that our `KMeans Clustering` does not do justice to these crescent shaped clusters in our data. This is because k-means, by design, looks for circular clusters in the data. 

Let's see if `Spectral Clustering` does a better job!

### About Notation

In all the math below: 

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors (1d arrays of numbers). 
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (`A@B`). 
$$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (`A@v`). 

&nbsp;

# Breaking the Algorithm Step by Step 

> ## Step 1: Create the Similarity Matrix 

- The `Similarity Matrix` $$\mathbf{A}$$ should be a matrix (2d `np.ndarray`) with shape `(n, n)` where `n` is the number of data points.

- An entry in $$\mathbf{A}$$ represented by `A[i,j]` is the distance between point `i` and `j`. Naturally `A[i,i]` should be 0.

- When constructing $$\mathbf{A}$$, we use a parameter `epsilon`. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`). Else, `A[i,j]` should be 0.


```python
def similarity_matrix(X, epsilon):
    
    # construct the pairwise distance matrix
    A = sklearn.metrics.pairwise_distances(X, metric='euclidean')
    
    # make the entries 1 or 0 if the condition is satisfied
    for col in range(A.shape[1]):
        A[:,col][A[:,col] < epsilon**2] = 1
        A[:,col][A[:,col] != 1] = 0
    
    return A
        
    
A = similarity_matrix(X, 0.4)
A
```
    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           ...,
           [0., 0., 0., ..., 0., 0., 1.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 1., 0., 0.]])
            0.        ]])


&nbsp;

> ## Step 2: Calculating Binary Norm Cut Objective 

The matrix $$\mathbf{A}$$ now contains information about which points are near (within distance `epsilon`) which other points. We now have to clustering the data points in `X` by partitioning the rows and columns of $$\mathbf{A}$$. 

&nbsp;

### Important Variables:

- Let's assume we have to clusters in our data: $$C_0$$ and $$C_1$$. We can say that every data point in $$\mathbf{X}$$ is either in $$C_0$$ or $$C_1$$. 

- Let $$d_i = \sum_{j = 1}^n a_{ij}$$. Thus, $$d_i$$ represents the sum of each row of $$\mathbf{A}$$.

- Let `y` be the vector of cluster labels where $$y[i]$$ is the cluster label (0 or 1) of point `i` in $$\mathbf{X}$$

&nbsp;

### The Binary Norm Cut Objective:

The *binary norm cut objective* of a matrix $$\mathbf{A}$$ is the function:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 

- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$ is the *cut* of the clusters $$C_0$$ and $$C_1$$. 


{::options parse_block_html="true" /}
<div class="red-box">

We want $$\mathbf{cut}(C_0, C_1)$$ to be 0 which means the points in the two clusters are seperated neatly or at a greater distance than `epsilon` from each other. 

$$\mathbf{cut}(C_0, C_1) \neq 0$$ if there are some points that may be close (i.e. distance between them is less than epsilon) to points from other clusters.

</div>
{::options parse_block_html="false" /}


- $$\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ 

{::options parse_block_html="true" /}
<div class="red-box">

The *volume* of cluster $$C_0$$ is a measure of the size of the cluster. 

</div>
{::options parse_block_html="false" /}

&nbsp;


```python
def cut(A,y):
    
    cut = 0 
    c1_idx = np.where(y == 0)[0] # the points in cluster 1
    c2_idx = np.where(y == 1)[0] # the points in cluster 2
    
    for row in c1_idx:
        for col in c2_idx:
            cut += A[row,col] # add the distance between points in the 2 clusters
            
    return cut
```

We can test `cut` of our dataset with two clusters against two randomly generated clusters.
We will notice that the `cut` for the true clusters is much smaller than the `cut` for the random clusters.

```python
true_cut = cut(A,y) # the true cut

random_y = np.random.randint(2, size = 200) # create two random clusters
random_cut = cut(A,random_y) # the random cut

print(f"Cut for true labels: {true_cut} \nCut for random labels: {random_cut}")
```

    Cut for true labels: 0.0 
    Cut for random labels: 397.0

Our assumption holds true. Moreover, notice that the `cut` for the true labels is 0; this means that for the given `epsilon = 0.4` the data can be clustered perfectly into two. 


```python
# computing the volumes 
def vols(A,y):
    
    c1_idx = np.where(y == 0)[0] # points in C_0
    
    c2_idx = np.where(y == 1)[0] # points in C_1
     
    vol0 = np.sum([np.sum(A[row,:]) for row in np.where(y == 0)[0]]) # vol_C0
    vol1 = np.sum([np.sum(A[row,:]) for row in np.where(y == 1)[0]]) # vol_C1
    
    return tuple([vol0,vol1])

# computing the binary norm cut objective   
def normcut(A,y):
    
    vol0 = vols(A,y)[0]
    vol1 = vols(A,y)[1]
    normcut = cut(A,y) * (1/vol0 + 1/vol1)
    return normcut
```

Now, let's compare the `normcut` objective using both the true labels `y` and the fake labels we generated above. 

```python
print(f"Normcut for true labels: {normcut(A,y)} \nNormcut for random labels: {normcut(A,random_y)}")
```

    Normcut for true labels: 1.5875649591888283 
    Normcut for random labels: 2.0228324293266664


{::options parse_block_html="true" /}
<div class="red-box">


We want the `binary normcut objective` to be as small as possible which would mean:

1. There are relatively few entries of $$\mathbf{A}$$ that join $$C_0$$ and $$C_1$$. 
2. Neither $$C_0$$ and $$C_1$$ are too small. 

</div>
{::options parse_block_html="false" /}


&nbsp;

###  Time for some Quick Math

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $$A$$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$$$


Note that the signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

Also, this holds true:

$$$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

*TO DO*:

1. We will write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 

2. We will check the equation above that relates the matrix product to the normcut objective by computing each side separately and checking that they are equal. 



```python
# 1
def transform(A,y):
    
    z = np.zeros(y.shape[0]) 
    z[np.where(y == 0)[0]] = 1/vols(A,y)[0]
    z[np.where(y == 1)[0]] = (-1)/vols(A,y)[1]
    
    return z
```


```python
# 2
z = transform(A,y)
D = np.zeros((n, n))
d = [np.sum(A[row,:]) for row in range(A.shape[0])]
np.fill_diagonal(D, d)

a = 2*(z.T @ (D - A) @ z)/(z.T @ D @ z)
print(a)
print(normcut(A,y))
```

    2.412442057241412
    1.5875649591888283


{::options parse_block_html="true" /}
<div class="red-box">

This means our new function is an *approximation* of the `binary norm cut objective`.

</div>
{::options parse_block_html="false" /}


&nbsp;

> ## Step 3: Optimization 

Since the our new function is a good approximation of `binary norm cut objective`, we can solve the problem of minimizing the normcut objective by minimizing the new function: 

$$$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$$$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

We will use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize

z_min = minimize(orth_obj, z).x  # the optimized cluster labels
```

&nbsp;

> ## Step 4: Plot the Clusters!


Recall that only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

```python
z_min[z_min >= 0] = 0
z_min[z_min < 0] = 1

plt.scatter(X[:,0], X[:,1], c = z_min)
```

![title](/images/output_9_1.png)
    

{::options parse_block_html="true" /}
<div class="red-box">

Hold on, we are not done yet!

- Explicitly optimizing the orthogonal objective is too slow for practical application. 

- The reason that `Spectral Clustering` actually matters, and indeed the reason that spectral clustering is called *spectral*, is that we can solve the optimization problem from Step 3 using eigenvalues and eigenvectors of matrices. 

</div>
{::options parse_block_html="false" /}

&nbsp;

> ## Optimization with Eigenvalues and Eigenvectors


Recall that what we would like to do is minimize the function 

$$$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$$$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$$$

which is equivalent to the standard eigenvalue problem 

$$$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$$$



Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

We will construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $$$$\mathbf{A}$$$$$$$$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, plot the data again, using the sign of `z_eig` as the color


```python
L = np.linalg.inv(D) @ (D -  A) # the Laplassian matrix

eig = np.sort(np.linalg.eig(L)[0])[1] # get the second smallest eigenvalue

eigenvalues = np.linalg.eig(L)[0] # array of all eigenvalues

idx = np.where(eigenvalues == eig)[0] # get the index for the second smallest eigenvalue

z_eigvec = np.linalg.eig(L)[1][:,idx] # get the eigenvector w.r.t eig

# plot the data
z_eigvec[z_eigvec >= 0] = 1
z_eigvec[z_eigvec < 0] = 0

plt.scatter(X[:,0], X[:,1], c = z_eigvec)
```

{::options parse_block_html="true" /}
<div class="red-box">

- <b> `np.linalg.inv()`</b>: calculates the inverse of the input matrix

- <b>`np.lanal.eig()`</b>: compute the eigenvalues and corresponding eigenvectors of the matrix 

- <b>`np.where()`</b>: get the index where the condition is satisfied

</div>
{::options parse_block_html="false" /}


![title](/images/output_9_1.png)
    
&nbsp;

# Combining Everything Together 

Let's make one function that performs all the previous steps and returns a clustered plot of our data!


```python
def spectral_clustering(X, epsilon):

    """
    -------
    PURPOSE
    To perfrom spectral clustering on the input data with the given epsilon parameter.

    ----------
    PARAMETERS
    1. X: the data, (n x 2) array 
    2. epsilon: the distance critera for two pooints to be considered in the same cluster.

    ------
    RETURN
    Array containing cluster labels obtained through Spectral Clustering.

    """
    
     # create the similarity matrix
    A = sklearn.metrics.pairwise_distances(X, metric='euclidean')
    
    for col in range(A.shape[1]):
        A[:,col][A[:,col] < epsilon**2] = 1
        A[:,col][A[:,col] != 1] = 0
        
    np.fill_diagonal(A, 0)
    
    # create the diagonal matrix
    D = np.zeros((n, n))
    d = [np.sum(A[row,:]) for row in range(A.shape[0])]
    np.fill_diagonal(D, d)
    
    # create the Laplacian matrix
    L = np.linalg.inv(D) @ (D -  A)
    
    # computing the eigenvector for the second-smallest eigenvalue
    eig_val = np.sort(np.linalg.eig(L)[0])[1] # the second-smallest eigenvalue
    eig_vals = np.linalg.eig(L)[0]            # the vector of eigenvalues
    iidx = np.where(eig_vals == eig_val)[0]   # get the index of the second-smallest eigenvalue
    z_eig = np.linalg.eig(L)[1][:,idx]        # get the eigenvector for eig_val
    
    z_eig[z_eig >= 0] = 1
    z_eig[z_eig < 0] = 0
    
    return z_eig
```

# Using `Spectral_Clustering()`

Let's compare *Spectral Clustering* to *K-means Clustering* on a 2 disk pattern.

```python
# make the data
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```

![png](/images/output_43_1.png)


```python
# perfrom k-means clustering
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```

![png](/images/output_45_1.png)


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```

![png](/images/output_46_1.png)

{::options parse_block_html="true" /}
<div class="red-box">

### Awesome! Our *Spectral Clustering* does much better than *K-means Clustering* on more 'weird' data.

</div>
{::options parse_block_html="false" /}

